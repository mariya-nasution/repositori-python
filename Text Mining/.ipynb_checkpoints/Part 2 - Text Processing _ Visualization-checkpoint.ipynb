{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "\n",
    "**Text processing** adalah suatu teori maupun praktik manipulasi maupun pembuatan teks.\n",
    "\n",
    "Beberapa tugas **text processing** diantaranya:\n",
    "\n",
    "- pencarian dan pertukaran\n",
    "- format\n",
    "- generate a processed report of the content of, or\n",
    "- filter a file or report of a text file.\n",
    "\n",
    "\n",
    "Berikut adalah beberapa library yang dibutuhkan untuk melakukan text processing\n",
    "\n",
    "- ```numpy```\n",
    "- ```pandas```\n",
    "- ```nltk``` (Natural Language Toolkit)\n",
    "- ```hunspell```\n",
    "- ```Sastrawi```\n",
    "- ```Scikit-Learn```\n",
    "- ```spacy```\n",
    "- ```gensim```\n",
    "\n",
    "## Instalasi Library\n",
    "\n",
    "Untuk instalasi masing-masing package dapat menjalankan script berikut melalui command-prompt atau bash\n",
    "\n",
    "```bash\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install nltk\n",
    "pip install hunspell\n",
    "pip install Sastrawi\n",
    "pip install scikit-learn\n",
    "pip install gensim\n",
    "pip install spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Preprocessing\n",
    "\n",
    "## Lowercasing & Uppercasing\n",
    "\n",
    "Lowercasing berfungsi untuk membuat setiap huruf pada suatu teks menjadi *lowercase*. Untuk mengubah teks menjadi *lowercase*, kita cukup menggunakan method ```lower``` pada teks. Sedangkan untuk mengubah teks menjadi *uppercase* cukup menggunakan method ```upper```.\n",
    "\n",
    "**Contoh:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "teks = 'Facebook resmi menutup 69 akun Facebook, 42 halaman Facebook, dan 34 akun Instagram dari Indonesia karena terindikasi melakukan \"perilaku tidak otentik yang terkoordinasi\". Sebagian besar akun tersebut mengangkat isu Papua yang ditulis dalam bahasa Indonesia dan bahasa Inggris.'\n",
    "\n",
    "# sebelum lowercasing\n",
    "teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setelah lowercasing\n",
    "teks.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setelah uppercase\n",
    "teks.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenisasi adalah proses memecah teks menjadi beberapa kalimat ataupun kata-kata terpisah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sent_tokenize, word_tokenize dan wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita akan mencoba melakukan tokenisasi pada salah satu paragraf dari berita berikut dari situs mojok.co: [INSIGHTID, PERUSAHAAN MISTERIUS YANG TERHUBUNG DENGAN RATUSAN AKUN PROPAGANDA PAPUA\n",
    "](https://mojok.co/red/rame/kilas/insightid-perusahaan-misterius-yang-terhubung-dengan-ratusan-akun-propaganda-papua/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teks = \"Facebook resmi menutup 69 akun Facebook, 42 halaman Facebook, dan 34 akun Instagram dari Indonesia karena terindikasi melakukan “perilaku tidak otentik yang terkoordinasi”. Sebagian besar akun tersebut mengangkat isu Papua yang ditulis dalam bahasa Indonesia dan bahasa Inggris.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(text = teks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_tokenize(text = teks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordpunct_tokenize(text = teks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menggunakan Modul re (Regular Expression)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'\\W+', teks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuang Tanda Baca (Punctuation)\n",
    "\n",
    "Tanda baca biasanya tidak memiliki nilai apapun dalam pengolahan teks, apalagi jika menggunakan metode klasik seperti *wordcount* atau *TF-IDF*, maka dari itu biasanya tanda baca seperti titik (.), koma (,) dan tanda baca lainnya mesti dibuang.\n",
    "\n",
    "Untuk membuang tanda baca kita dapat menggunakan method ```replace``` dan menggunakan ReGex seperti berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hasil_tokenisasi = wordpunct_tokenize(text = teks)\n",
    "hasil_tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "hasil_tokenisasi = [hasil for hasil in hasil_tokenisasi if hasil not in list(string.punctuation)]\n",
    "\n",
    "' '.join(hasil_tokenisasi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming adalah proses menghilangkan *afix* atau imbuhan yang tujuannya agar memperoleh kata dasar\n",
    "\n",
    "**Menggunakan Sastrawi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load StemmerFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Membuat Steammer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya kita akan mencoba melakukan stemming pada kata-kata berikut:\n",
    "\n",
    "- kemalaman\n",
    "- kesiangan\n",
    "- berkawan\n",
    "- permainan\n",
    "- keterlaluan\n",
    "- keberpihakan\n",
    "- mempertanggungjawabkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daftar_kata = [\"kemalaman\", \n",
    "               \"kesiangan\", \n",
    "               \"berkawan\", \n",
    "               \"permainan\", \n",
    "               \"keterlaluan\", \n",
    "               \"keberpihakan\", \n",
    "               \"mempertanggungjawabkan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for kata in daftar_kata:\n",
    "    print(stemmer.stem(kata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming bisa dilakukan per teks seperti berikut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(teks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menggunakan Hunspell**\n",
    "\n",
    "**Note**:\n",
    "\n",
    "Untuk menggunakan hunspell, dibutuhkan beberapa library tambahan, misalnya:\n",
    "\n",
    "- Untuk Linux, wajib mendownload ```libhunspell-dev```\n",
    "- Untuk windows, bisa mendownload dari https://pyhunspell.latinier.fr/ dan install dari source\n",
    "- Untuk MacOS, harus instalasi dari source di github: https://github.com/blatinier/pyhunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import hunspell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menggunakan library hunspell, wajib mengunduh dictionary khusus tergantung dari bahasa yang digunakan, misalnya untuk bahasa Indonesia bisa di https://cgit.freedesktop.org/libreoffice/dictionaries/plain/id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat objek hunspell dengan menggunakan dictionary bahasa indonesia\n",
    "hunspell_object = hunspell.HunSpell('./dictionary/id_ID.dic','./dictionary/id_ID.aff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# melakukan stemming per kata\n",
    "for kata in daftar_kata:\n",
    "    print(hunspell_object.stem(kata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Stopwords secara sederhana dapat didefinisikan sebagai kata-kata yang tidak relevan, biasanya sering muncul dalam sebuah teks seperti kata 'adalah', 'ini', 'itu' dan sejenisnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "# Membuat Stopword Remover\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords = factory.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melihat daftar stopwords\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teks sebelum stopwords\n",
    "teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_tokenisasi = word_tokenize(teks)\n",
    "hasil_tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# melihat hasil setelah stopwords dibuang dari teks\n",
    "[hasil for hasil in hasil_tokenisasi if not hasil in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Feature Extraction\n",
    "\n",
    "## Wordcount\n",
    "\n",
    "Kita dapat menghitung jumlah kata dengan cara menggunakan ```FreqDist``` dari NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load package\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist(hasil_tokenisasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(FreqDist(hasil_tokenisasi).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(FreqDist(hasil_tokenisasi).items()), columns=['Kata', 'Frekuensi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters\n",
    "\n",
    "Untuk mengektrak jumlah karakter dalam sebuah teks, kita dapat menggunakan fungsi ```len```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(teks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Character\n",
    "\n",
    "Untuk menghitung jumlah karakter spesial tertentu seperti \"#\", \"@\" dan lainnya, kita dapat menggunakan cara berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([hasil for hasil in hasil_tokenisasi if hasil in list(string.punctuation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "N-grams adalah pasangan kata terdiri dari N kata.\n",
    "\n",
    "- N=1 dikatakan sebagai unigram\n",
    "- N=2 dikatakan sebagai bigram\n",
    "- N=3 dikatakan sebagai trigram\n",
    "\n",
    "dan seterusnya. Biasanya berfungsi untuk mengekstrak struktur dari teks atau kata yang terdiri dari 2 atau lebih kata seperti \"Rumah Sakit\", \"Tempat Tidur\", \"Sepeda Motor\" dan sebagainya.\n",
    "\n",
    "Untuk mengekstrak N-gram, kita dapat menggunakan method ```ngrams``` dari ```nltk```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pasangan_kata in ngrams(teks.split(),2):\n",
    "    print(pasangan_kata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordclouds\n",
    "\n",
    "Untuk melakukan visualisasi pada data teks, kita bisa menggunakan Wordclouds untuk melihat kata apa saja yang memiliki frekuensi kemunculan terbesar. Biasanya worcloud digunakan untuk mencari kata-kata kunci dari sebuah dokumen atau teks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(teks.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate_from_frequencies(dict(FreqDist(hasil_tokenisasi).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barchart\n",
    "\n",
    "Karena data teks dapat dianggap sebagai data kategorik, kita dapat memperhitungkan frekuensi tiap kata dengan menggunakan barchart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frekuensi_kata = list(FreqDist([hasil.lower() for hasil in hasil_tokenisasi]).items())\n",
    "frekuensi_kata = pd.DataFrame(frekuensi_kata, columns=['Kata', 'Frekuensi'])\n",
    "frekuensi_kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frekuensi_kata.sort_values(by='Frekuensi', ascending=True).plot.barh(x='Kata', y='Frekuensi', figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frekuensi_kata.sort_values(by='Frekuensi', ascending=False).nlargest(n = 10, columns='Frekuensi').plot.barh('Kata', 'Frekuensi');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
